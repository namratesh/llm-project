{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTE9KUaKs4KS"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install diffusers transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "LkBFDd9oto2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# llama"
      ],
      "metadata": {
        "id": "gnRe3rFDuleK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "hf_token = userdata.get('hf_token')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "51Hw2mRguqnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "m-sXDS4PuG6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "tokens = tokenizer.encode(text)\n",
        "print(\"Encoded tokens\", tokens)\n",
        "print(\"decode tokens\", tokenizer.decode(tokens))\n",
        "print(\"decode tokens in batch\", tokenizer.batch_decode(tokens))\n"
      ],
      "metadata": {
        "id": "d1VpZnZsuoCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models that are trained in chat version called instruct variants models"
      ],
      "metadata": {
        "id": "lXboGqHpveHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "apDk5qhIu4h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "GGbnvL5bvQWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phi"
      ],
      "metadata": {
        "id": "dwJgdp0evtD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "tokens = phi3_tokenizer.encode(text)\n",
        "print(phi3_tokenizer.batch_decode(tokens))\n"
      ],
      "metadata": {
        "id": "AsfnSQi_vpNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XoqDAjkbvweg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}